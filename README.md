# Prometheus 
## Overview of Prometheus

- [Brainstorm] What does a data model for a monitoring system mean?
    
    Data model represent the data structure used by the monitoring system to store.organize and manipulate data.

- [Brainstorm] Are traditional monitoring systems are Push based or Pull based? What does that really mean - can you explain to someone who is completely new to this whole thing?

    Traditional monitoring sytems are push based where the data source initiates the data transfer by pushing the data to the monitoring system. In pull based system the monitoring system pulls/scrap the data from the data source. This approach is useful when the data source is able to provide data on demand.

- [Brainstorm] So what could be various components of a monitoring system - like a storage system and a server and agent etc. what else? What would you do if you were to create a monitoring system from scratch?

    There are number of different components that can be included in monitoring system depend on the goal and requirment of the system 
        
    1. Data sources: These are the sources of data that the monitoring system collects and processes.

    2. Data collection and transmission components: These components are responsible for collecting data from the data sources and transmitting it to the monitoring system. This can include agents, gateways, or other software or hardware components that are installed on or near the data sources.

    3. Data storage and management components: These components are responsible for storing and organizing the data collected by the monitoring system. This can include a database, a data warehouse, or other storage and management systems.

    4. Data processing and analysis components: These components are responsible for processing and analyzing the data collected by the monitoring system. This can include analytics engines, machine learning models, or other tools and algorithms that are used to extract insights and knowledge from the data.

    5. User interface and visualization components: These components provide a way for users to interact with the monitoring system and view the data and insights generated by the system. 

    6. Alerting component : This is used for creating alerts based on the data collected.


### Assignment

- Download and run Prometheus on local machine: 
---
ANS:

- Create the user and group that will be used by  prometheus application
  
    ```bash
     sudo useradd -M -r -s /bin/false prometheus
    ```
- Create the following directories 

    ```bash
    sudo mkdir /etc/prometheus /var/lib/prometheus
    ```
- Download prometheus archive and extract it

  ```bash
  wget https://github.com/prometheus/prometheus/releases/download/v2.41.0/prometheus-2.41.0.linux-amd64.tar.gz

  tar xvfz prometheus-*.tar.gz
  ```
- Copy the following binaries from the extracted archives and change the ownership as mentioned below

    ```bash
    sudo cp {prometheus,promtool} /usr/local/bin/
    sudo chown prometheus:prometheus /usr/local/bin/{prometheus,promtool}
    ```
- Copy console,console_libraries files and directories to /etc/prometheus from extracted archive

    ```bash
    sudo cp -r {consoles,console_libraries}  /etc/prometheus 
    ```
- Copy the prometheus config file to /etc/prometheus and change the ownership of /etc/prometheus and /var/lib/prometheus as follows

    ```bash
    sudo cp prometheus.yml /etc/prometheus
    sudo chown -R prometheus:prometheus /etc/prometheus
    sudo chown  prometheus:prometheus /var/lib/prometheus
    ```
- Create the systemd unit file `/etc/systemd/system/prometheus.service` for prometheus with following configuration

    ```bash
    [Unit]
    Description=Prometheus Time Series Collection and Processing Server
    Wants=network-online.target
    After=network-online.target
    [Service]
    User=prometheus
    Start and enable the Prometheus service:
    Make an HTTP request to Prometheus to verify it is able to respond:
    You can also access Prometheus in a browser using the server's public IP address: ht
    tp://<PROMETHEUS_SERVER_PUBLIC_IP>:9090.
    Group=prometheus
    Type=simple
    ExecStart=/usr/local/bin/prometheus \
    --config.file /etc/prometheus/prometheus.yml \
    --storage.tsdb.path /var/lib/prometheus/ \
    --web.console.templates=/etc/prometheus/consoles \
    --web.console.libraries=/etc/prometheus/console_libraries
    [Install]
    WantedBy=multi-user.target
    ```
- Start and enable the Prometheus service:

    ```bash
    sudo systemctl daemon-reload
    sudo systemctl start prometheus
    sudo systemctl enable prometheus
    ```
- Make an HTTP request to Prometheus to verify it is able to respond 
    ```bash
    curl localhost:9090
    ```

---
- Once Prometheus is up and running, explore the the UI in the browser

---

- Create a graph of the metric you are trying to monitor

    ANS:

    1. Access the promethues : `http://localhost:9090/graph`

    2. Enter the query `up` in the expression browser and click on graph tab
    
    insert scree shot
- Get metrics of node on which you are running Prometheus by running node exporter

    ANS:

    1. Download the archive and extract it 

        ```bash
        wget https://github.com/prometheus/node_exporter/releases/download/v1.5.0/node_exporter-1.5.0.linux-amd64.tar.gz

        tar -xzvf node_exporter-*.*.tar.gz
        ```
    
    2. Start the node exporter 

        ```bash
        cd node_exporter-*.*

        ./node_exporter 
        ```
       Note: By default node exporter uses 9100 port
    3. Add the following job definition to the scrape_configs section in prometheus.yml and restart your Prometheus instance:

        ```yaml
        scrape_configs:
           - job_name: 'node'
             scrape_interval: 5s
             static_configs:
              - targets: ['localhost:9100']
        ```


-  Try out an example of a more complex query

   ANS:
        
    Tried out the following query in the expression browser
    ```
    rate(node_cpu_seconds_total[5m])
    ```
---
- Brainstorm
   
   1. What does scraping even mean?
      
      It is the process of collecting metrics from the target
      Prometheus periodically retrieves metrics from the configured targets using HTTP request, which are called scraps 
  2. Why did we need a node exporter - did they not say Prometheus is an agent-less and pull based system? If so what is difference between an agent and a exporter? Wait, what is even an exporter?

       Prometheus does not require any agents to be installed on the target systems that you want to monitor. Instead, Prometheus periodically sends HTTP requests to the targets to collect metrics, which are then stored in a time-series database.

       An exporter is a piece of software that runs on a target system and exposes metrics about that system to be scraped by Prometheus

       The exporters only collect the metrics and it wont push it to the prometheus. Prometheus need to scrap it from the exporters

    3.  Where is Promethus storing data on my local machine when I ran it?? I am running out of storage alreday

        By default, Prometheus stores its data in a local time-series database in a file on your machine. The location of this file is specified by the `--storage.tsdb.path` flag when you start Prometheus. If you did not specify this flag, the default location for the data file is `./data/.`

        If you are running out of storage, you can either delete some of the data that Prometheus has collected by using the TSDB delete command or you can increase the amount of storage available to Prometheus by specifying a different value for the --storage.tsdb.path flag.

        It is also possible to configure Prometheus to store its data in a remote time-series database, such as InfluxDB, instead of using a local file-based database. This can be useful if you want to store a large amount of data or if you want to share the data with other systems.

    4. Does Prometheus run always on a instance directly? What other means you could install it with?

        Prometheus typically runs as a standalone instance on a host machine, either as a binary that you run directly on the command line, or as a system service that runs in the background.

        There are several ways that you can install and run Prometheus on a host machine:
        
         1. Binary
         2. Docker
         3. Kubernetes: You can deploy Prometheus on a Kubernetes cluster using the Prometheus Operator
    
    5.  I thought they said prometheus was meant for "cloud native monitoring" then why it is running as a simple binary? Why not some complex way of installing it?

        The choice of how to install and run Prometheus will depend on your specific needs and the complexity of your environment. Whether you run Prometheus as a simple binary on a single host, or deploy it as a containerized application in a distributed environment, the core functionality of Prometheus remains the same.
    6. Is a single binary a good thing or a bad thing?

        There are pros and cons to running Prometheus as a single binary on a single host. Some advantages of this approach include:

         1. Simplicity
         2. Ease of deployment
        
        disadvantages:
        
         1. Resource usage: 
          
            Because Prometheus runs on a single host, it can consume a significant amount of CPU, memory, and storage resources. This may be an issue if you are monitoring a large number of targets or if you have a large volume of metrics to store.
         2. Availability: If the host machine that is running Prometheus goes down or becomes unavailable, your monitoring system will also be unavailable. 

    7. What is bare minimum configuration needed for running Prometheus?
       
       To run Prometheus, we need to provide a configuration file that specifies the target systems that you want to monitor and the metrics that you want to collect from them.

       A minimal configuration file is described below
       ```
       global:
            scrape_interval: 15s

       scrape_configs:
          - job_name: my_target
            static_configs:
                - targets: ['localhost:9090']
       ```
    
    8.  How is auth achieved in config file?

        The authentiation details can be configured in the Prometheus configuration file.Prometheus supports basic authentication and TLS. 
    9. They have ec2_sd_config and azure_sd_config and such for cloud providers - what does it do exactly?

        Service discovery is a feature of Prometheus that allows it to automatically discover and monitor targets.he ec2_sd_config and azure_sd_config options are used to configure the integration of Prometheus with the Amazon EC2  and Azure cloud platforms, respectively.
        When you enable service discovery for a cloud platform, Prometheus will use the cloud provider's APIs to discover and monitor the target systems running in that platform. This can be useful if you are running a large number of dynamic targets in the cloud and you want to automate the process of monitoring them.
        
        To use service discovery with a cloud provider, you will need to provide the necessary authentication credentials and configure the relevant service discovery options in the Prometheus configuration file. For example, to enable EC2 service discovery, you can use the ec2_sd_config option in the scrape_configs section of the configuration file, like this:

        ```
        scrape_configs:
          - job_name: my_cloud_targets
            ec2_sd_configs:
                - region: us-west-2
                  access_key: YOUR_ACCESS_KEY
                  secret_key: YOUR_SECRET_KEY
        ```
    
    10. How is Kubernetes_sd_config different or similar to cloud provider sd configs?

        When you enable service discovery for Kubernetes, Prometheus will use the Kubernetes API to discover and monitor the target systems running in the cluster. This can be useful if you are running a large number of dynamic targets in Kubernetes and you want to automate the process of monitoring them.
---

## Getting deeper into Prometheus & intro to PromQL

- [Brainstorm] How can you filter some applications when querying with PromQL?

   we can use labels and operators for filtering 

- [Brainstorm] How can I measure a metric over last 30 minutes?

   we can use `range vector selector`  for this.  The range vector selector allows us to specify a time range and select the samples for a metric within that range.

- What is a subquery and in what situations can I find it useful?

  In PromQL, a subquery is a query that is embedded within another query

  some situations where subqueries might be useful:

  1. Calculating derived metrics: You can use a subquery to calculate derived metrics, which are metrics that are derived from other metrics. For example, you might use a subquery to calculate the average request latency of a web server by dividing the total request latency by the number of requests:
        ```  
        avg(latency_seconds / requests)
        ```
  2. Comparing multiple metrics: You can use a subquery to compare multiple metrics by selecting the metrics and applying a function to them. For example, you might use a subquery to compare the request latencies of two different web servers by selecting the latency_seconds metric for each server and applying the avg() function:

        ```
        avg(latency_seconds{server="web1"}) / avg(latency_seconds{server="web2"})
        ```

  3. Combining multiple queries: You can use a subquery to combine multiple queries and perform more complex analysis on your metrics. For example, you might use a subquery to calculate the average request latency of a web server over the last hour, and then compare it to the average request latency over the last day:


        ```
        avg(latency_seconds[1h]) / avg(latency_seconds[24h])
        ```

## Grafana

- [Brainstorm] What other data sources Grafana supports?
   
   1. InfluxDB
   2. Elasticsearch
   3. Graphite
   4. Loki
   5. PostgreSQL
   6. MySQL etc.

#### Brainstorm
---
- What are all these kinds of visualizations - like heatmap, geomap, candlestick, Bar gauge - I have never used these, what are they used for?

   1. Heatmap: A heatmap is a graphical representation of data where the individual values contained in a matrix are represented as colors. Heatmaps are often used to visualize data that has a large number of values, such as log data or performance metrics, and to identify patterns and trends in the data.

   2. Geomap: A geomap is a map-based visualization that is used to display data on a geographical map. Geomaps are often used to visualize data that has a geographical component, such as the location of servers or the origin of web traffic.

   3. Candlestick: A candlestick chart is a type of financial chart that is used to visualize the price movement of a security, commodity, or currency. Candlestick charts consist of a series of "candlesticks" that show the opening, closing, high, and low prices for a given time period.

   4. Bar gauge: A bar gauge is a type of visualization that is used to display a single value as a horizontal or vertical bar. Bar gauges are often used to display progress, status, or other values that are represented as a percentage or a ratio.

- What are typical metrics we monitor for a microservice and what kinds of visualizations will make sense?

  There are many metrics that we want to monitor for a microservice, depending on the specific requirements of our application and the type of microservice we are running. some of them are

  1. Request and response metrics:  metrics like request rate, request latency, and error rate
  2. Resource utilization metrics:  metrics like CPU usage, memory usage, and network usage,
  3. Service-level metrics: metrics like uptime, availability, and reliability

- What are typical metrics we monitor for a database or a message queue- basically stateful applications and what kinds of visualizations will make sense?

  1. Performance metrics: metrics like query rate, query latency, and throughput
  2. Replication and availability metrics: metrics like replication lag, failover time, and failover count

---


## Alert Manager 

- [Brainstorm] Why are there two types of rules in first place, what purpose they serve?

Recording rules are used to pre-process and filter data .This can be useful for transforming the data into a more suitable format for querying and visualizing.

Alerting rules are used to trigger alerts based on the value of certain metric time series.

- What things should go in recording rules vs. what should go in alerting rules? Are there any rules for that?

If we want to pre-process some quires at regular intervals we can use recording rules and if we want to create an alerts when some threshold reaches we can use alerting rule



## Master Assignment

I have used my local kind cluster for completing this Assignment

### Prerequisite
- Create the namespace and crd,prometheus operator by applying the manifests in crd and operator folder

- Run Node Exporter on local machine:

  Apply the manifests in NodeExporter folder

- Run Prometheus on local machine

  1. Apply the manifests in Prometheus folder, this will also run Thanos as a side car 

- Download and run Thanos Querier, configure it to add the Thanos Sidecar as a store

  1. Apply the manifests in thanos folder

- Run Grafana on local machine 
  1. Apply the manifests in grafana folder , this will also create the dashboards for monitoring Disk Usage,CPU Usage, Network Usage
